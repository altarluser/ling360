{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "52fac150-4f4b-4c3d-8ab4-93ee8d5ef100"
      },
      "source": [
        "# <font color='#FFBD33'>**Assignment 5 - Pop Song Generator/Spelling Correction**</font> \n",
        "\n",
        "This is the <font color='cyan'>Assignment 5</font> for the LING360 - Computational Methods in Lingustics course and it is worth a total of  <font color='cyan'>**10 points**</font>.\n",
        "The assignment covers edit distance and its utilization. \n",
        "\n",
        "The topics include:\n",
        "1. N-grams\n",
        "\n",
        "\n",
        "There's a total of  <font color='cyan'>**2 main tasks**</font> and <font color='cyan'>**7 subs tasks**</font>. For each task, please write your code between the following lines:\n",
        "\n",
        "```\n",
        "## YOUR CODE STARTS\n",
        "\n",
        "\n",
        "\n",
        "## YOUR CODE ENDS\n",
        "```\n",
        "\n",
        "Before working on the assignment, please copy this notebook to your own drive. You can use ```Save a copy in Drive``` under the ```File``` menu on top left.\n",
        "\n",
        "Please, run every cell in your code to make sure that it works properly before submitting it. \n",
        "\n",
        "Once you are ready to submit, download two versions of your code:\n",
        "\n",
        "*   Download .ipynb\n",
        "*   Download .py\n",
        "\n",
        "These are both available under the ```File``` menu on top left. \n",
        "\n",
        "Then, compress your files (zip, rar, or whatever) and upload the compressed file to Moodle.\n",
        "\n",
        "If you have any questions, please contact with karahan.sahin@boun.edu.tr\n"
      ],
      "id": "52fac150-4f4b-4c3d-8ab4-93ee8d5ef100"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y1C7nPd52yG1"
      },
      "outputs": [],
      "source": [
        "## USE THIS IF YOU ARE NOT ABLE TO GENERATE UNIGRAMS/BIGRAMS IN FIRST QUESTION\n",
        "\n",
        "## FIRST GET FILES AND ADD THEM TO THE SESSION FILES\n",
        "# LINK: https://drive.google.com/drive/folders/1CgdUd70XfCu1ttY3UEWLtif9v5c14aAm?usp=sharing\n",
        "import pickle \n",
        "\n",
        "with open(\"unigrams.pkl\", \"rb\") as ff:\n",
        "    unigram_counts = pickle.load(ff)\n",
        "\n",
        "\n",
        "with open(\"bigrams.pkl\", \"rb\") as ff:\n",
        "    bigram_counts = pickle.load(ff)"
      ],
      "id": "Y1C7nPd52yG1"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "836b159b-983a-4807-a97f-9a61b827579e"
      },
      "source": [
        "## <font color='#FFBD33'>**Q1:** Pop Song Generator</font> `5 points`\n",
        "\n",
        "Generate unique pop song using turkish songs dataset and n-grams."
      ],
      "id": "836b159b-983a-4807-a97f-9a61b827579e"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "cd9dcfd3-dc6a-4ecb-b833-1a3e2c395c1b"
      },
      "outputs": [],
      "source": [
        "!pip install editdistance --quiet"
      ],
      "id": "cd9dcfd3-dc6a-4ecb-b833-1a3e2c395c1b"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3MOos_Evkn7x",
        "outputId": "8e3770a2-fb80-4bdf-cb7c-1c98e55a8b2b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "## RUN THIS LINE FIRST\n",
        "import re\n",
        "import editdistance\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "id": "3MOos_Evkn7x"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b4d66d52-4f9c-46a5-b402-6d5aba102978"
      },
      "source": [
        "### <font color='#FFBD33'>**Q1.1:** Clean/Tokenize Corpus</font> `1.5 Points`\n",
        "\n",
        "Tokenize all the words in your corpus. While tokenizing the data, remove tokens which are only consist of numbers, punctuations, or single letter. \n",
        "\n",
        "<font color='#FFBD33'>**Instructions:**</font>\n",
        "\n",
        "1. Write function called `importCorpus()` with parameter called `artist_names`\n",
        "1. Import and read file `turkish_pop_songs.txt` using `open()` function. File structure is like this:\n",
        "    ```txt\n",
        "    artist_name     song_lyric....\n",
        "    artist_name     song_lyric....\n",
        "    .....\n",
        "    ```\n",
        "    \n",
        "1. Start reading line by line, which you can either use `readlines()` function or read all and split the string from `\\n` characters via `split()` function.\n",
        "1. If `artist_name` is equal to `\"all\"`, get all the lyrics. (not the artist_name, only the lyrics)\n",
        "2. If `artist_name` is a list containing artist names such as `[\"Tarkan\", \"Gülşen\", ..]`, get only the lyrics belong to the artist via `if` statement.\n",
        "1. After getting all the lyrics, tokenize each lyric with `nltk.word_tokenize()` function\n",
        "1. Add `<START>` token to the beginning of the token list and `<END>` token to the ending of the token list.\n",
        "1. Append the lyrics it to a variable called `corpus`\n",
        "1. Finally return `corpus`.\n",
        "\n",
        "<font color='#FFBD33'>**Notes:**</font>\n",
        "\n",
        "1. Don't forget to open your file with `encoding=\"utf-8\"` parameter.\n",
        "1. `artist_name` and `song_lyrics` are separated with tab character shown as `\\t` character. Don't forget to split each line from `\\t` character before processing the line\n",
        "1. **Hint**:\n",
        "    ```python\n",
        "    [\"hello\"]+[\"world\"]\n",
        "    # Output: [\"hello\", \"world\"]\n",
        "    ```\n"
      ],
      "id": "b4d66d52-4f9c-46a5-b402-6d5aba102978"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "9d188661-788a-45df-8cf3-5c10a230dded"
      },
      "outputs": [],
      "source": [
        "def importCorpus(artist_name=\"all\"):\n",
        "    \"\"\"The function for \n",
        "    \n",
        "    Args:\n",
        "        artist_name (str or list): list containing artist names or \"all\" value for selection of lyrics\n",
        "    \n",
        "    Returns:\n",
        "        corpus (list): list of lists containing token lyrics \n",
        "        \n",
        "    \"\"\"\n",
        "    ## YOUR CODE STARTS\n",
        "    with open(\"turkish_pop_songs.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "     temp_lines = f.read().split(\"\\n\")\n",
        "    \n",
        "    lyrics = []\n",
        "    lines = []\n",
        "\n",
        "    for line in temp_lines:\n",
        "      splitted_line = line.split(\"\\t\")\n",
        "      lines.append(splitted_line)\n",
        "\n",
        "    if artist_name == \"all\":\n",
        "      for line in lines:\n",
        "        if len(line) >= 2:\n",
        "          lyrics.append(line[1])\n",
        "    \n",
        "    else:\n",
        "      if isinstance(artist_name, str):\n",
        "        for line in lines:\n",
        "            if len(line) >= 2 and artist_name == line[0]:\n",
        "              lyrics.append(line[1])    \n",
        "\n",
        "      elif isinstance(artist_name, list):\n",
        "        for artist in artist_name:\n",
        "          for line in lines:\n",
        "            if len(line) >= 2 and artist == line[0]:\n",
        "              lyrics.append(line[1])\n",
        "\n",
        "    corpus = []\n",
        "    for lyric in lyrics:\n",
        "      lyric_tokens = nltk.word_tokenize(lyric)\n",
        "      lyric_tokens = [\"<START>\"] + [token for token in lyric_tokens if token.isalpha() and len(token) > 1] + [\"<END>\"]\n",
        "      corpus.append(lyric_tokens)\n",
        "    ## YOUR CODE ENDS\n",
        "    \n",
        "    return corpus"
      ],
      "id": "9d188661-788a-45df-8cf3-5c10a230dded"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dffd59b6-edc9-48ff-a3ea-0bf57086d06c"
      },
      "source": [
        "### <font color='#FFBD33'>**Q1.2:** Uni-Gram Songs</font> `0.5 Points`\n",
        "\n",
        "Extract the uni-grams for given artist_names. \n",
        "\n",
        "<font color='#FFBD33'>**Instructions:**</font>\n",
        "\n",
        "1. First import your corpus using function called `importCorpus()` with parameter called `artist_names` given to your `getUnigrams()`.\n",
        "1. Then, start extract your unigrams with the code we have seen in Lecture 8.\n",
        "1. Finally return the unigram dictionary."
      ],
      "id": "dffd59b6-edc9-48ff-a3ea-0bf57086d06c"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "uaz0i-EgNrYm"
      },
      "outputs": [],
      "source": [
        "from itertools import chain \n",
        "from collections import Counter"
      ],
      "id": "uaz0i-EgNrYm"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "7a19aba9-86f5-4df3-9226-8f35e52dc41b"
      },
      "outputs": [],
      "source": [
        "def getUnigrams(corpus):\n",
        "    \"\"\"Get unigrams for given artist_names\n",
        "    \n",
        "    Args:\n",
        "        corpus (list): list of lists containing token lyrics \n",
        "\n",
        "    Returns:\n",
        "        unigram_counts (dictionary): dictionary containing unigrams\n",
        "    \"\"\"\n",
        "\n",
        "    ## YOUR CODE STARTS\n",
        "    corpus = importCorpus()\n",
        "\n",
        "    unigram_counts = Counter(chain(*[x for x in corpus]))\n",
        "  \n",
        "    ## YOUR CODE ENDS    \n",
        "    \n",
        "    return unigram_counts"
      ],
      "id": "7a19aba9-86f5-4df3-9226-8f35e52dc41b"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_CUkDrSaL_eS"
      },
      "outputs": [],
      "source": [
        "corpus = importCorpus(artist_name=\"all\")\n",
        "getUnigrams(corpus)"
      ],
      "id": "_CUkDrSaL_eS"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "58623038-f385-4d68-abfa-6c01f3fba243"
      },
      "source": [
        "### <font color='#FFBD33'>**Q1.3:** Bi-Gram Songs</font> `0.5 Points`\n",
        "\n",
        "Extract the bi-grams for given artist_names. \n",
        "\n",
        "<font color='#FFBD33'>**Instructions:**</font>\n",
        "\n",
        "1. First import your corpus using function called `importCorpus()` with parameter called `artist_names` given to your `getBigrams()`.\n",
        "1. Then, start extract your unigrams with the code we have seen in Lecture 8.\n",
        "1. Finally return the bigram dictionary."
      ],
      "id": "58623038-f385-4d68-abfa-6c01f3fba243"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "ba8fcafc-3340-4be3-9474-063bf0d85d3d"
      },
      "outputs": [],
      "source": [
        "\n",
        "def getBigrams(corpus):\n",
        "    \"\"\"Get bigrams for given artist_names\n",
        "    \n",
        "    Args:\n",
        "        corpus (list): list of lists containing token lyrics \n",
        "\n",
        "    Returns:\n",
        "        bigram_counts (dictionary): dictionary containing bigrams\n",
        "    \"\"\"\n",
        "    ## YOUR CODE STARTS\n",
        "    corpus = importCorpus()\n",
        "    corpus = [i for i in chain(*[x for x in corpus])]\n",
        "    bigrams = [(corpus[i], corpus[i+1]) for i in range(len(corpus)-1)]\n",
        "\n",
        "    bigram_counts = Counter(bigrams)\n",
        "    \n",
        "    ## YOUR CODE ENDS    \n",
        "    \n",
        "    return bigram_counts"
      ],
      "id": "ba8fcafc-3340-4be3-9474-063bf0d85d3d"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u8QTPHZTty3S"
      },
      "outputs": [],
      "source": [
        "corpus = importCorpus(artist_name=['Büyük Ev Ablukada', 'Ceza', 'Ezhel', 'Mor Ve Ötesi', 'Sagopa Kajmer', 'Serdar Ortaç', 'Teoman'])\n",
        "getBigrams(corpus)"
      ],
      "id": "u8QTPHZTty3S"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d346d915-e6f4-4008-a7f4-5ff0ef18b754"
      },
      "source": [
        "### <font color='#FFBD33'>**Q1.4:** Tri-Gram Songs</font> `0.5 Points`\n",
        "\n",
        "Extract the tri-grams for given artist_names. \n",
        "\n",
        "<font color='#FFBD33'>**Instructions:**</font>\n",
        "\n",
        "1. First import your corpus using function called `importCorpus()` with parameter called `artist_names` given to your `getTrigrams()`.\n",
        "1. Then, start extract your trigrams with the code we have seen in Lecture 8.\n",
        "1. Finally return the unigram dictionary."
      ],
      "id": "d346d915-e6f4-4008-a7f4-5ff0ef18b754"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "7eff1e46-257e-40fe-a6a8-354530a6c15c"
      },
      "outputs": [],
      "source": [
        "\n",
        "def getTrigrams(corpus):\n",
        "    \"\"\"Get trigrams for given artist_names\n",
        "    \n",
        "    Args:\n",
        "        corpus (list): list of lists containing token lyrics \n",
        "\n",
        "    Returns:\n",
        "        grams (dictionary): dictionary containing bigrams\n",
        "    \"\"\"\n",
        "    \n",
        "    ## YOUR CODE STARTS\n",
        "    corpus = importCorpus()\n",
        "    corpus = [i for i in chain(*[x for x in corpus])]\n",
        "    \n",
        "    trigrams = [((corpus[i],corpus[i+1]), corpus[i+2]) for i in range(len(corpus)-2)]\n",
        "\n",
        "    trigram_counts = Counter(trigrams)\n",
        "    \n",
        "    ## YOUR CODE ENDS    \n",
        "    \n",
        "    return trigram_counts\n"
      ],
      "id": "7eff1e46-257e-40fe-a6a8-354530a6c15c"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uscc37FOtpxO"
      },
      "outputs": [],
      "source": [
        "corpus = importCorpus(artist_name=['Can Bonomo', 'Cem Adrian', 'Duman', 'Mabel Matiz', 'Mazhar Fuat Özkan (MFÖ)', 'Mor Ve Ötesi', 'Mustafa Sandal', 'Pinhani', 'Serdar Ortaç', 'Sertab Erener', 'Tarkan', 'Teoman', 'Yaşar', ])\n",
        "getTrigrams(corpus)"
      ],
      "id": "Uscc37FOtpxO"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "68092f84-3685-472e-9eb5-a18086a08ef2"
      },
      "source": [
        "### <font color='#FFBD33'>**Q1.5:** Final Generator</font> `2 Points`\n",
        "\n",
        "This is the final generator where you need to generate your song with given artist_names.\n",
        "\n",
        "<font color='#FFBD33'>**Instructions:**</font>\n",
        "\n",
        "1. First get artist names for generation and number of grams between 1-3\n",
        "2. Get the corresponding `n_gram` corpus according to correct `n_count`  where `n_count=1` is unigrams, `n_count=2` is bigrams and so on..\n",
        "4. After that updating start generating songs until, you have reached `<END>` token or reached `max_token_count`\n",
        "5. Joining your tokens into one string with `.join(\" \")` method.\n",
        "5. Replace the token `\\\\n` with `\\n` character with using `re.sub(\"\\\\n\",\"\\n\", lyrics)` function.\n",
        "5. Return the song by joining your tokens into one string.\n",
        "\n",
        "\n",
        "<font color='#FFBD33'>**Notes:**</font>\n",
        "1. Initialize generation process from `<START>` token.\n",
        "1. If you want to use `max_token_count` count, count your token where generation process."
      ],
      "id": "68092f84-3685-472e-9eb5-a18086a08ef2"
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "_yIG9792ZCYu"
      },
      "outputs": [],
      "source": [
        "from scipy import stats\n",
        "from collections import defaultdict"
      ],
      "id": "_yIG9792ZCYu"
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "22d77e43-0bfc-4aa6-9229-987273ddc307"
      },
      "outputs": [],
      "source": [
        "\n",
        "def SongGenerator(artist_names=\"all\", n_count=2, max_token_count=200):\n",
        "    \"\"\"Generate song for given artist names\n",
        "        \n",
        "    Args:\n",
        "        artist_name (str or dict): list containing artist names or \"all\" value for selection of lyrics\n",
        "        n_count (int): number of grams which will be generated, values: (1 == \"unigram\"), (2 == \"bigram\"), (3 == \"trigram\")\n",
        "        max_token_count (int): maximum number of tokens will be generated excluding \"<START>\" and \"<END>\" tokens.\n",
        "    \n",
        "    Returns:\n",
        "        lyrics (str): ngram generated string\n",
        "    \n",
        "    \"\"\"\n",
        "    \n",
        "    ## YOUR CODE STARTS\n",
        "    artist_names = input(\"Write artist_names or all. If you want to write more than one, make it sure it is a list \")\n",
        "    n_count = int(input(\"number of grams (1-3) \"))\n",
        "\n",
        "    corpus = importCorpus(artist_names)\n",
        "    lyrics = []\n",
        "\n",
        "    if n_count == 1:\n",
        "      \n",
        "      text = []\n",
        "      counter = 0\n",
        "\n",
        "      unigram_counts = getUnigrams(corpus)\n",
        "      unigram_probs = defaultdict(dict)\n",
        "      unigram_probs = {key:value/len(unigram_counts.items()) for key, value in unigram_counts.items()}\n",
        "      sorted_unigram_probs = list(sorted(unigram_probs.items(), key=lambda x: x[1], reverse=True))\n",
        "      \n",
        "      for i in range(300):\n",
        "        counter += 1\n",
        "        current_word = sorted_unigram_probs[i][0]\n",
        "        text.append(current_word)\n",
        "        if counter == max_token_count:\n",
        "          break\n",
        "\n",
        "      lyrics = \" \".join(text[1:-1])\n",
        "      lyrics = re.sub(\"\\\\n\", \"\\n\", lyrics)\n",
        "\n",
        "    elif n_count == 2:\n",
        "      text = []\n",
        "      counter = 0\n",
        "      bigram_counts = getBigrams(corpus)\n",
        "      bigram_freq = nltk.ConditionalFreqDist(bigram_counts)\n",
        "      bigram_prob = nltk.ConditionalProbDist(bigram_freq, nltk.MLEProbDist)\n",
        "\n",
        "      current_word = \"<START>\"\n",
        "\n",
        "      for i in range(300):\n",
        "        counter += 1\n",
        "        probable_words = list(bigram_prob[current_word].samples())\n",
        "        word_probabilities = [bigram_prob[current_word].prob(word) for word in probable_words]\n",
        "        result = stats.multinomial.rvs(1,word_probabilities)\n",
        "        index_of_probable_word = list(result).index(1)\n",
        "        current_word = probable_words[index_of_probable_word]\n",
        "        text.append(current_word)\n",
        "        if current_word == \"<END>\" or counter == max_token_count:\n",
        "          break\n",
        "      \n",
        "      lyrics = \" \".join(text[1:-1])\n",
        "      lyrics = re.sub(\"\\\\n\", \"\\n\", lyrics)\n",
        "\n",
        "    elif n_count == 3:\n",
        "      text = []\n",
        "      counter = 0\n",
        "      trigram_counts = getTrigrams(corpus)\n",
        "      trigram_freq = nltk.ConditionalFreqDist(trigram_counts)\n",
        "      trigram_prob = nltk.ConditionalProbDist(trigram_freq, nltk.MLEProbDist)\n",
        "\n",
        "      current_phrase = ('<START>','Ben')\n",
        "      print(current_phrase[1], end= \" \")\n",
        "      for i in range(300):\n",
        "        counter += 1\n",
        "        probable_words = list(trigram_prob[current_phrase].samples())\n",
        "        word_probabilities = [trigram_prob[current_phrase].prob(word) for word in probable_words]\n",
        "              \n",
        "        result = stats.multinomial.rvs(1,word_probabilities)\n",
        "        index_of_probable_word = list(result).index(1)\n",
        "        current_phrase = (current_phrase[1],(probable_words[index_of_probable_word]))\n",
        "        text.append(current_phrase[1])\n",
        "        if current_phrase[1] == \"<END>\" or counter == max_token_count:\n",
        "          break\n",
        "\n",
        "      lyrics = \" \".join(text[1:-1])\n",
        "      lyrics = re.sub(\"\\\\n\", \"\\n\", lyrics)\n",
        "\n",
        "    ## YOUR CODE ENDS\n",
        "    \n",
        "    return lyrics"
      ],
      "id": "22d77e43-0bfc-4aa6-9229-987273ddc307"
    },
    {
      "cell_type": "code",
      "source": [
        "SongGenerator()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 103
        },
        "id": "z38gt3PlZYOV",
        "outputId": "e6fb4082-7960-4641-9cfb-e5b532ec5ce2"
      },
      "id": "z38gt3PlZYOV",
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Write artist_names or all. If you want to write more than one, make it sure it is a list Ahmet Kaya\n",
            "number of grams (1-3) 3\n",
            "Ben "
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'deli Yaşarız geceleri Yıldızlar söner An olur canım Sen beni bilemedin yüreğimi göremedin Kendini bilemedin yamacıma gelemedin Amacına varamadın her yeri holocaust görünüz gösteriniz İşte bu maç bak yandı Kavruldu pota ve parkeler Haydi bi daha Hırslıyım kaybettiğim şeyler var daha Seninle yaşamaya Vallahi çok yakında Seninle yaşamaya'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7e26872d-1000-450c-9c25-81c1b07c56d8"
      },
      "source": [
        "## <font color='#FFBD33'>**Q2:** Multi-Word Auto Correction</font> `5 points`\n",
        "\n",
        "In the previous assignment, we have seen the auto-correction system for single words. But for multi-word sentence, how can we know which corrections is the most likely to happen?\n",
        "Well, n-grams are coming handy for finding best possible combination. This model is [Noisy Channel](https://en.wikipedia.org/wiki/Noisy_channel_model) model in literature which is used in various applications such as Speech-to-Text, Machine Translation models. Although, the application is not the final noisy channel model, yet it provides the foundation of the model.\n",
        "\n",
        "In this application, we will be finding the best sequence of corrections for a given incorrect spelling within a sentence.\n"
      ],
      "id": "7e26872d-1000-450c-9c25-81c1b07c56d8"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e060af58-c3e5-4423-98b3-d7c06fdd6045"
      },
      "source": [
        "### <font color='#FFBD33'>**Q2.1:** Recommend with Edit Distance</font> `1.5 Points`\n",
        "\n",
        "Write your suggestion function using editdistance algorithm.\n",
        "\n",
        "<font color='#FFBD33'>**Instructions:**</font>\n",
        "1. First, check whether you have the word in your `unigram_counts` dictionary. If it is, return a list containing only `word`.\n",
        "1. If it is not available in your corpus, check the words edit distance of each word in your `unigram_counts` dictionary via `for` loop.\n",
        "1. After saving edit distance of each word in your `unigram_counts` dictionary and given word, find the minimum edit distance and assign it to a variable named `min_dist` \n",
        "1. Define a empty list called `suggestions`.\n",
        "1. The iterate over `sorted_unigrams` and add suggestions to `suggestions` list until list has length of `n_suggestions`.\n",
        "1. Finally return `suggestions` list"
      ],
      "id": "e060af58-c3e5-4423-98b3-d7c06fdd6045"
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "5afe6b79-90fe-4131-8d88-3875e8887c52"
      },
      "outputs": [],
      "source": [
        "def getSuggestionsWithEditDistance(word, n_suggestions):\n",
        "\n",
        "    ## YOUR CODE STARTS\n",
        "    unigram_counts = getUnigrams(\"all\")\n",
        "    sorted_unigrams = list(sorted(unigram_counts.items(), key=lambda x: x[1], reverse=True))\n",
        "\n",
        "    user_word = []\n",
        "    if word in unigram_counts:\n",
        "      user_word.append(word)\n",
        "      return user_word\n",
        "    else:\n",
        "      distance_list = []\n",
        "      for unigram in unigram_counts:\n",
        "        distance = editdistance.distance(unigram[0], word)\n",
        "        distance_list.append(distance)\n",
        "\n",
        "        min_dist = min(distance_list)\n",
        "\n",
        "        suggestions = []\n",
        "        for candidate in sorted_unigrams:\n",
        "          if min_dist == editdistance.distance(candidate[0], word):\n",
        "            suggestions.append(candidate[0])\n",
        "            if len(suggestions) == n_suggestions:\n",
        "              break\n",
        "    \n",
        "    ## YOUR CODE ENDS\n",
        "    \n",
        "    return suggestions"
      ],
      "id": "5afe6b79-90fe-4131-8d88-3875e8887c52"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "79a15a25-aff0-4a34-a3a3-b911c3ad3e70"
      },
      "source": [
        "### <font color='#FFBD33'>**Q2.2:** Get All Suggestions for the Sentence</font> `3.5 Points`\n",
        "\n",
        "Calculate most probable corrections for the incorrect sentence.\n",
        "\n",
        "<font color='#FFBD33'>**Instructions:**</font>\n",
        "1. For a given sentence, get all corrections for each tokens and save it to list called `tokens`. \n",
        "1. Then get all combinations of corrections via `getAllCombinations()` function.\n",
        "1. Get the ngram corpus for all of the corpus with `importCorpus(\"all\")` and get bigram dictionary with `getBigrams(corpus)`\n",
        "1. For each possible sentence, calculate the total sum of ngram counts and save it into `ngram_scores` dictionary.\n",
        "1. Finally sort your dictionary with respect to `ngram_scores` and get the `0th` index key, and assign it to a variable called `best_sentence`.\n",
        "\n",
        "\n",
        "``` python\n",
        "#\"ali\", \"eve\", \"geldi\" -> (\"<START>\", \"ali\") = 2 + (\"ali\", \"eve\") = 1 + (\"eve\", \"geldi\") = 20 + (\"geldi\", \"<END>\") = 6 -> 29\n",
        "#\"ali\", \"evler\", \"geldi\" -> (\"<START>\", \"ali\") = 2 + (\"ali\", \"evler\") = 0 + (\"evler\", \"geldi\") = 0 + (\"geldi\", \"<END>\") = 6 -> 8 \n",
        "\n",
        "# Your dictionary should be like:\n",
        "ngram_scores = {\n",
        "    (\"ali\", \"eve\", \"geldi\"): 29,\n",
        "    (\"ali\", \"evler\", \"geldi\"): 8,\n",
        "    ...\n",
        "}\n",
        "```"
      ],
      "id": "79a15a25-aff0-4a34-a3a3-b911c3ad3e70"
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c44edbe1-1040-4f8e-9b7d-ebabb87d918d",
        "outputId": "21c8d2df-192c-4410-de54-878c7a98b833",
        "scrolled": true
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['<START>', 'ali', 'eve', 'gelir', '<END>'],\n",
              " ['<START>', 'ali', 'eve', 'gelsin', '<END>'],\n",
              " ['<START>', 'ali', 'eve', 'geldim', '<END>'],\n",
              " ['<START>', 'ali', 'evden', 'gelir', '<END>'],\n",
              " ['<START>', 'ali', 'evden', 'gelsin', '<END>'],\n",
              " ['<START>', 'ali', 'evden', 'geldim', '<END>'],\n",
              " ['<START>', 'ali', 'evler', 'gelir', '<END>'],\n",
              " ['<START>', 'ali', 'evler', 'gelsin', '<END>'],\n",
              " ['<START>', 'ali', 'evler', 'geldim', '<END>']]"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "from itertools import product\n",
        "\n",
        "def getAllCombinations(tokens): \n",
        "  return [list(i) for i in list(product(*tokens))]\n",
        "\n",
        "tokens = [ [\"<START>\"] ,[\"ali\"], [\"eve\", \"evden\", \"evler\"], [\"gelir\", \"gelsin\", \"geldim\"], [\"<END>\"] ]\n",
        "\n",
        "getAllCombinations(tokens)"
      ],
      "id": "c44edbe1-1040-4f8e-9b7d-ebabb87d918d"
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "780e38c8-c2ac-43dc-b5fe-83a1526af53a"
      },
      "outputs": [],
      "source": [
        "\n",
        "def getBestCorrections(sentence):\n",
        "    \"\"\"Calculates best possible combination of spelling correction using n-grams\n",
        "    \n",
        "    Args:\n",
        "        sentence (str): string of incorrect sentence\n",
        "\n",
        "    Args:\n",
        "        best_sentence (str): string of corrected sentence\n",
        "    \"\"\"\n",
        "    \n",
        "    ## YOUR CODE STARTS\n",
        "    tokenized_sentence = nltk.word_tokenize(sentence)\n",
        "    tokens = []\n",
        "    for token in tokenized_sentence:\n",
        "      candidates = tuple(getSuggestionsWithEditDistance(token, 3))\n",
        "      tokens.append(candidates)\n",
        "    \n",
        "    combinations = getAllCombinations(tokens)\n",
        "\n",
        "    ngrams = importCorpus(\"all\")\n",
        "    bigram_counts = getBigrams(\"all\")\n",
        "\n",
        "    ngram_scores = {}\n",
        "    for combination in combinations:\n",
        "      score = bigram_counts[(\"<START>\", combination[0])] + bigram_counts[(combination[-1], \"<END>\")]\n",
        "      for i in range(len(combination)-1):\n",
        "        score += bigram_counts[(combination[i], combination[i+1])]\n",
        "      ngram_scores[tuple(combination)] = score\n",
        "\n",
        "    best_sentence = list(sorted(ngram_scores.items(), key=lambda x: x[1], reverse=True))[0]\n",
        "    ## YOUR CODE ENDS\n",
        "    \n",
        "    return best_sentence"
      ],
      "id": "780e38c8-c2ac-43dc-b5fe-83a1526af53a"
    },
    {
      "cell_type": "code",
      "source": [
        "getBestCorrections(\"Ali evler geldi\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iBTgVQVFVy9R",
        "outputId": "c984813e-678b-4305-c9ca-404e0c5d4940"
      },
      "id": "iBTgVQVFVy9R",
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(('Ali', 'evler', 'geldi'), 7)"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}