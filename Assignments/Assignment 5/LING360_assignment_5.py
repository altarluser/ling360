# -*- coding: utf-8 -*-
"""AhmetAltarLuser_Assignment 5.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/19BBLk3qthtcm-4T8jtpBj0dpbJyv0Loy

# <font color='#FFBD33'>**Assignment 5 - Pop Song Generator/Spelling Correction**</font> 

This is the <font color='cyan'>Assignment 5</font> for the LING360 - Computational Methods in Lingustics course and it is worth a total of  <font color='cyan'>**10 points**</font>.
The assignment covers edit distance and its utilization. 

The topics include:
1. N-grams


There's a total of  <font color='cyan'>**2 main tasks**</font> and <font color='cyan'>**7 subs tasks**</font>. For each task, please write your code between the following lines:

```
## YOUR CODE STARTS



## YOUR CODE ENDS
```

Before working on the assignment, please copy this notebook to your own drive. You can use ```Save a copy in Drive``` under the ```File``` menu on top left.

Please, run every cell in your code to make sure that it works properly before submitting it. 

Once you are ready to submit, download two versions of your code:

*   Download .ipynb
*   Download .py

These are both available under the ```File``` menu on top left. 

Then, compress your files (zip, rar, or whatever) and upload the compressed file to Moodle.

If you have any questions, please contact with karahan.sahin@boun.edu.tr
"""

## USE THIS IF YOU ARE NOT ABLE TO GENERATE UNIGRAMS/BIGRAMS IN FIRST QUESTION

## FIRST GET FILES AND ADD THEM TO THE SESSION FILES
# LINK: https://drive.google.com/drive/folders/1CgdUd70XfCu1ttY3UEWLtif9v5c14aAm?usp=sharing
import pickle 

with open("unigrams.pkl", "rb") as ff:
    unigram_counts = pickle.load(ff)


with open("bigrams.pkl", "rb") as ff:
    bigram_counts = pickle.load(ff)

"""## <font color='#FFBD33'>**Q1:** Pop Song Generator</font> `5 points`

Generate unique pop song using turkish songs dataset and n-grams.
"""

!pip install editdistance --quiet

## RUN THIS LINE FIRST
import re
import editdistance
from nltk.tokenize import word_tokenize
import nltk
nltk.download('punkt')

"""### <font color='#FFBD33'>**Q1.1:** Clean/Tokenize Corpus</font> `1.5 Points`

Tokenize all the words in your corpus. While tokenizing the data, remove tokens which are only consist of numbers, punctuations, or single letter. 

<font color='#FFBD33'>**Instructions:**</font>

1. Write function called `importCorpus()` with parameter called `artist_names`
1. Import and read file `turkish_pop_songs.txt` using `open()` function. File structure is like this:
    ```txt
    artist_name     song_lyric....
    artist_name     song_lyric....
    .....
    ```
    
1. Start reading line by line, which you can either use `readlines()` function or read all and split the string from `\n` characters via `split()` function.
1. If `artist_name` is equal to `"all"`, get all the lyrics. (not the artist_name, only the lyrics)
2. If `artist_name` is a list containing artist names such as `["Tarkan", "Gülşen", ..]`, get only the lyrics belong to the artist via `if` statement.
1. After getting all the lyrics, tokenize each lyric with `nltk.word_tokenize()` function
1. Add `<START>` token to the beginning of the token list and `<END>` token to the ending of the token list.
1. Append the lyrics it to a variable called `corpus`
1. Finally return `corpus`.

<font color='#FFBD33'>**Notes:**</font>

1. Don't forget to open your file with `encoding="utf-8"` parameter.
1. `artist_name` and `song_lyrics` are separated with tab character shown as `\t` character. Don't forget to split each line from `\t` character before processing the line
1. **Hint**:
    ```python
    ["hello"]+["world"]
    # Output: ["hello", "world"]
    ```

"""

def importCorpus(artist_name="all"):
    """The function for 
    
    Args:
        artist_name (str or list): list containing artist names or "all" value for selection of lyrics
    
    Returns:
        corpus (list): list of lists containing token lyrics 
        
    """
    ## YOUR CODE STARTS
    with open("turkish_pop_songs.txt", "r", encoding="utf-8") as f:
     temp_lines = f.read().split("\n")
    
    lyrics = []
    lines = []

    for line in temp_lines:
      splitted_line = line.split("\t")
      lines.append(splitted_line)

    if artist_name == "all":
      for line in lines:
        if len(line) >= 2:
          lyrics.append(line[1])
    
    else:
      if isinstance(artist_name, str):
        for line in lines:
            if len(line) >= 2 and artist_name == line[0]:
              lyrics.append(line[1])    

      elif isinstance(artist_name, list):
        for artist in artist_name:
          for line in lines:
            if len(line) >= 2 and artist == line[0]:
              lyrics.append(line[1])

    corpus = []
    for lyric in lyrics:
      lyric_tokens = nltk.word_tokenize(lyric)
      lyric_tokens = ["<START>"] + [token for token in lyric_tokens if token.isalpha() and len(token) > 1] + ["<END>"]
      corpus.append(lyric_tokens)
    ## YOUR CODE ENDS
    
    return corpus

"""### <font color='#FFBD33'>**Q1.2:** Uni-Gram Songs</font> `0.5 Points`

Extract the uni-grams for given artist_names. 

<font color='#FFBD33'>**Instructions:**</font>

1. First import your corpus using function called `importCorpus()` with parameter called `artist_names` given to your `getUnigrams()`.
1. Then, start extract your unigrams with the code we have seen in Lecture 8.
1. Finally return the unigram dictionary.
"""

from itertools import chain 
from collections import Counter

def getUnigrams(corpus):
    """Get unigrams for given artist_names
    
    Args:
        corpus (list): list of lists containing token lyrics 

    Returns:
        unigram_counts (dictionary): dictionary containing unigrams
    """

    ## YOUR CODE STARTS
    corpus = importCorpus()

    unigram_counts = Counter(chain(*[x for x in corpus]))
  
    ## YOUR CODE ENDS    
    
    return unigram_counts

corpus = importCorpus(artist_name="all")
getUnigrams(corpus)

"""### <font color='#FFBD33'>**Q1.3:** Bi-Gram Songs</font> `0.5 Points`

Extract the bi-grams for given artist_names. 

<font color='#FFBD33'>**Instructions:**</font>

1. First import your corpus using function called `importCorpus()` with parameter called `artist_names` given to your `getBigrams()`.
1. Then, start extract your unigrams with the code we have seen in Lecture 8.
1. Finally return the bigram dictionary.
"""

def getBigrams(corpus):
    """Get bigrams for given artist_names
    
    Args:
        corpus (list): list of lists containing token lyrics 

    Returns:
        bigram_counts (dictionary): dictionary containing bigrams
    """
    ## YOUR CODE STARTS
    corpus = importCorpus()
    corpus = [i for i in chain(*[x for x in corpus])]
    bigrams = [(corpus[i], corpus[i+1]) for i in range(len(corpus)-1)]

    bigram_counts = Counter(bigrams)
    
    ## YOUR CODE ENDS    
    
    return bigram_counts

corpus = importCorpus(artist_name=['Büyük Ev Ablukada', 'Ceza', 'Ezhel', 'Mor Ve Ötesi', 'Sagopa Kajmer', 'Serdar Ortaç', 'Teoman'])
getBigrams(corpus)

"""### <font color='#FFBD33'>**Q1.4:** Tri-Gram Songs</font> `0.5 Points`

Extract the tri-grams for given artist_names. 

<font color='#FFBD33'>**Instructions:**</font>

1. First import your corpus using function called `importCorpus()` with parameter called `artist_names` given to your `getTrigrams()`.
1. Then, start extract your trigrams with the code we have seen in Lecture 8.
1. Finally return the unigram dictionary.
"""

def getTrigrams(corpus):
    """Get trigrams for given artist_names
    
    Args:
        corpus (list): list of lists containing token lyrics 

    Returns:
        grams (dictionary): dictionary containing bigrams
    """
    
    ## YOUR CODE STARTS
    corpus = importCorpus()
    corpus = [i for i in chain(*[x for x in corpus])]
    
    trigrams = [((corpus[i],corpus[i+1]), corpus[i+2]) for i in range(len(corpus)-2)]

    trigram_counts = Counter(trigrams)
    
    ## YOUR CODE ENDS    
    
    return trigram_counts

corpus = importCorpus(artist_name=['Can Bonomo', 'Cem Adrian', 'Duman', 'Mabel Matiz', 'Mazhar Fuat Özkan (MFÖ)', 'Mor Ve Ötesi', 'Mustafa Sandal', 'Pinhani', 'Serdar Ortaç', 'Sertab Erener', 'Tarkan', 'Teoman', 'Yaşar', ])
getTrigrams(corpus)

"""### <font color='#FFBD33'>**Q1.5:** Final Generator</font> `2 Points`

This is the final generator where you need to generate your song with given artist_names.

<font color='#FFBD33'>**Instructions:**</font>

1. First get artist names for generation and number of grams between 1-3
2. Get the corresponding `n_gram` corpus according to correct `n_count`  where `n_count=1` is unigrams, `n_count=2` is bigrams and so on..
4. After that updating start generating songs until, you have reached `<END>` token or reached `max_token_count`
5. Joining your tokens into one string with `.join(" ")` method.
5. Replace the token `\\n` with `\n` character with using `re.sub("\\n","\n", lyrics)` function.
5. Return the song by joining your tokens into one string.


<font color='#FFBD33'>**Notes:**</font>
1. Initialize generation process from `<START>` token.
1. If you want to use `max_token_count` count, count your token where generation process.
"""

from scipy import stats
from collections import defaultdict

def SongGenerator(artist_names="all", n_count=2, max_token_count=200):
    """Generate song for given artist names
        
    Args:
        artist_name (str or dict): list containing artist names or "all" value for selection of lyrics
        n_count (int): number of grams which will be generated, values: (1 == "unigram"), (2 == "bigram"), (3 == "trigram")
        max_token_count (int): maximum number of tokens will be generated excluding "<START>" and "<END>" tokens.
    
    Returns:
        lyrics (str): ngram generated string
    
    """
    
    ## YOUR CODE STARTS
    artist_names = input("Write artist_names or all. If you want to write more than one, make it sure it is a list ")
    n_count = int(input("number of grams (1-3) "))

    corpus = importCorpus(artist_names)
    lyrics = []

    if n_count == 1:
      
      text = []
      counter = 0

      unigram_counts = getUnigrams(corpus)
      unigram_probs = defaultdict(dict)
      unigram_probs = {key:value/len(unigram_counts.items()) for key, value in unigram_counts.items()}
      sorted_unigram_probs = list(sorted(unigram_probs.items(), key=lambda x: x[1], reverse=True))
      
      for i in range(300):
        counter += 1
        current_word = sorted_unigram_probs[i][0]
        text.append(current_word)
        if counter == max_token_count:
          break

      lyrics = " ".join(text[1:-1])
      lyrics = re.sub("\\n", "\n", lyrics)

    elif n_count == 2:
      text = []
      counter = 0
      bigram_counts = getBigrams(corpus)
      bigram_freq = nltk.ConditionalFreqDist(bigram_counts)
      bigram_prob = nltk.ConditionalProbDist(bigram_freq, nltk.MLEProbDist)

      current_word = "<START>"

      for i in range(300):
        counter += 1
        probable_words = list(bigram_prob[current_word].samples())
        word_probabilities = [bigram_prob[current_word].prob(word) for word in probable_words]
        result = stats.multinomial.rvs(1,word_probabilities)
        index_of_probable_word = list(result).index(1)
        current_word = probable_words[index_of_probable_word]
        text.append(current_word)
        if current_word == "<END>" or counter == max_token_count:
          break
      
      lyrics = " ".join(text[1:-1])
      lyrics = re.sub("\\n", "\n", lyrics)

    elif n_count == 3:
      text = []
      counter = 0
      trigram_counts = getTrigrams(corpus)
      trigram_freq = nltk.ConditionalFreqDist(trigram_counts)
      trigram_prob = nltk.ConditionalProbDist(trigram_freq, nltk.MLEProbDist)

      current_phrase = ('<START>','Ben')
      print(current_phrase[1], end= " ")
      for i in range(300):
        counter += 1
        probable_words = list(trigram_prob[current_phrase].samples())
        word_probabilities = [trigram_prob[current_phrase].prob(word) for word in probable_words]
              
        result = stats.multinomial.rvs(1,word_probabilities)
        index_of_probable_word = list(result).index(1)
        current_phrase = (current_phrase[1],(probable_words[index_of_probable_word]))
        text.append(current_phrase[1])
        if current_phrase[1] == "<END>" or counter == max_token_count:
          break

      lyrics = " ".join(text[1:-1])
      lyrics = re.sub("\\n", "\n", lyrics)

    ## YOUR CODE ENDS
    
    return lyrics

SongGenerator()

"""## <font color='#FFBD33'>**Q2:** Multi-Word Auto Correction</font> `5 points`

In the previous assignment, we have seen the auto-correction system for single words. But for multi-word sentence, how can we know which corrections is the most likely to happen?
Well, n-grams are coming handy for finding best possible combination. This model is [Noisy Channel](https://en.wikipedia.org/wiki/Noisy_channel_model) model in literature which is used in various applications such as Speech-to-Text, Machine Translation models. Although, the application is not the final noisy channel model, yet it provides the foundation of the model.

In this application, we will be finding the best sequence of corrections for a given incorrect spelling within a sentence.

### <font color='#FFBD33'>**Q2.1:** Recommend with Edit Distance</font> `1.5 Points`

Write your suggestion function using editdistance algorithm.

<font color='#FFBD33'>**Instructions:**</font>
1. First, check whether you have the word in your `unigram_counts` dictionary. If it is, return a list containing only `word`.
1. If it is not available in your corpus, check the words edit distance of each word in your `unigram_counts` dictionary via `for` loop.
1. After saving edit distance of each word in your `unigram_counts` dictionary and given word, find the minimum edit distance and assign it to a variable named `min_dist` 
1. Define a empty list called `suggestions`.
1. The iterate over `sorted_unigrams` and add suggestions to `suggestions` list until list has length of `n_suggestions`.
1. Finally return `suggestions` list
"""

def getSuggestionsWithEditDistance(word, n_suggestions):

    ## YOUR CODE STARTS
    unigram_counts = getUnigrams("all")
    sorted_unigrams = list(sorted(unigram_counts.items(), key=lambda x: x[1], reverse=True))

    user_word = []
    if word in unigram_counts:
      user_word.append(word)
      return user_word
    else:
      distance_list = []
      for unigram in unigram_counts:
        distance = editdistance.distance(unigram[0], word)
        distance_list.append(distance)

        min_dist = min(distance_list)

        suggestions = []
        for candidate in sorted_unigrams:
          if min_dist == editdistance.distance(candidate[0], word):
            suggestions.append(candidate[0])
            if len(suggestions) == n_suggestions:
              break
    
    ## YOUR CODE ENDS
    
    return suggestions

"""### <font color='#FFBD33'>**Q2.2:** Get All Suggestions for the Sentence</font> `3.5 Points`

Calculate most probable corrections for the incorrect sentence.

<font color='#FFBD33'>**Instructions:**</font>
1. For a given sentence, get all corrections for each tokens and save it to list called `tokens`. 
1. Then get all combinations of corrections via `getAllCombinations()` function.
1. Get the ngram corpus for all of the corpus with `importCorpus("all")` and get bigram dictionary with `getBigrams(corpus)`
1. For each possible sentence, calculate the total sum of ngram counts and save it into `ngram_scores` dictionary.
1. Finally sort your dictionary with respect to `ngram_scores` and get the `0th` index key, and assign it to a variable called `best_sentence`.


``` python
#"ali", "eve", "geldi" -> ("<START>", "ali") = 2 + ("ali", "eve") = 1 + ("eve", "geldi") = 20 + ("geldi", "<END>") = 6 -> 29
#"ali", "evler", "geldi" -> ("<START>", "ali") = 2 + ("ali", "evler") = 0 + ("evler", "geldi") = 0 + ("geldi", "<END>") = 6 -> 8 

# Your dictionary should be like:
ngram_scores = {
    ("ali", "eve", "geldi"): 29,
    ("ali", "evler", "geldi"): 8,
    ...
}
```
"""

from itertools import product

def getAllCombinations(tokens): 
  return [list(i) for i in list(product(*tokens))]

tokens = [ ["<START>"] ,["ali"], ["eve", "evden", "evler"], ["gelir", "gelsin", "geldim"], ["<END>"] ]

getAllCombinations(tokens)

def getBestCorrections(sentence):
    """Calculates best possible combination of spelling correction using n-grams
    
    Args:
        sentence (str): string of incorrect sentence

    Args:
        best_sentence (str): string of corrected sentence
    """
    
    ## YOUR CODE STARTS
    tokenized_sentence = nltk.word_tokenize(sentence)
    tokens = []
    for token in tokenized_sentence:
      candidates = tuple(getSuggestionsWithEditDistance(token, 3))
      tokens.append(candidates)
    
    combinations = getAllCombinations(tokens)

    ngrams = importCorpus("all")
    bigram_counts = getBigrams("all")

    ngram_scores = {}
    for combination in combinations:
      score = bigram_counts[("<START>", combination[0])] + bigram_counts[(combination[-1], "<END>")]
      for i in range(len(combination)-1):
        score += bigram_counts[(combination[i], combination[i+1])]
      ngram_scores[tuple(combination)] = score

    best_sentence = list(sorted(ngram_scores.items(), key=lambda x: x[1], reverse=True))[0]
    ## YOUR CODE ENDS
    
    return best_sentence

getBestCorrections("Ali evler geldi")